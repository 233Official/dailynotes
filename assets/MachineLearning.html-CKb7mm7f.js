import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,a,o as n}from"./app-DX9gUdjC.js";const l={};function r(o,e){return n(),t("div",null,e[0]||(e[0]=[a('<h2 id="_2-3-特征工程" tabindex="-1"><a class="header-anchor" href="#_2-3-特征工程"><span>2.3-特征工程</span></a></h2><hr><h3 id="_2-3-2-特征选择" tabindex="-1"><a class="header-anchor" href="#_2-3-2-特征选择"><span>2.3.2-特征选择</span></a></h3><ul><li>特征选择的目的主要是降维，从特征集合中挑选一组最具统计意义的特征子集来代表整体样本的特点。特征选择的方法是用一些评价指标单独地计算出各个特征与目标变量之间的关系。常见的有Pearson相关系数、基尼指标(Gini index )、信息增益(Information Gain )等 ，下面以Pearson相 关系数为例，它的计算方式如下 <ul><li><img src="http://cdn.ayusummer233.top/img/20210509171131.png" alt="20210509171131"></li></ul></li></ul><hr><h3 id="_2-3-3-特征提取" tabindex="-1"><a class="header-anchor" href="#_2-3-3-特征提取"><span>2.3.3-特征提取</span></a></h3><hr><h3 id="类别可分离性判据" tabindex="-1"><a class="header-anchor" href="#类别可分离性判据"><span>类别可分离性判据</span></a></h3><ul><li>衡量不同特征及其组合是否有效的的定量准则</li><li>应满足条件： <ul><li>度量特性：不同类大于零 同类等于零</li><li>与错误率单调关系</li><li>特征独立时有可加性</li><li>单调性：特征越多，盘踞越大</li></ul></li><li>分类： <ul><li>基于距离</li><li>概率</li><li>熵函数</li></ul></li><li>分类特征要求 <ul><li>类别可分性</li><li>可靠性</li><li>特征间独立性</li><li>数量尽量少</li></ul></li></ul>',9)]))}const m=i(l,[["render",r],["__file","MachineLearning.html.vue"]]),p=JSON.parse('{"path":"/AI/%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/MachineLearning.html","title":"","lang":"zh-CN","frontmatter":{"article":false,"description":"2.3-特征工程 2.3.2-特征选择 特征选择的目的主要是降维，从特征集合中挑选一组最具统计意义的特征子集来代表整体样本的特点。特征选择的方法是用一些评价指标单独地计算出各个特征与目标变量之间的关系。常见的有Pearson相关系数、基尼指标(Gini index )、信息增益(Information Gain )等 ，下面以Pearson相 关系数为...","head":[["meta",{"property":"og:url","content":"https://233official.github.io/dailynotes/AI/%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%9F%BA%E7%A1%80/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/MachineLearning.html"}],["meta",{"property":"og:site_name","content":"DailyNotes"}],["meta",{"property":"og:description","content":"2.3-特征工程 2.3.2-特征选择 特征选择的目的主要是降维，从特征集合中挑选一组最具统计意义的特征子集来代表整体样本的特点。特征选择的方法是用一些评价指标单独地计算出各个特征与目标变量之间的关系。常见的有Pearson相关系数、基尼指标(Gini index )、信息增益(Information Gain )等 ，下面以Pearson相 关系数为..."}],["meta",{"property":"og:type","content":"website"}],["meta",{"property":"og:image","content":"http://cdn.ayusummer233.top/img/20210509171131.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2026-01-26T06:11:52.000Z"}],["meta",{"property":"article:modified_time","content":"2026-01-26T06:11:52.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"WebPage\\",\\"name\\":\\"\\",\\"description\\":\\"2.3-特征工程 2.3.2-特征选择 特征选择的目的主要是降维，从特征集合中挑选一组最具统计意义的特征子集来代表整体样本的特点。特征选择的方法是用一些评价指标单独地计算出各个特征与目标变量之间的关系。常见的有Pearson相关系数、基尼指标(Gini index )、信息增益(Information Gain )等 ，下面以Pearson相 关系数为...\\"}"]],"date":"2022-11-07T17:17:10.000Z"},"headers":[{"level":2,"title":"2.3-特征工程","slug":"_2-3-特征工程","link":"#_2-3-特征工程","children":[{"level":3,"title":"2.3.2-特征选择","slug":"_2-3-2-特征选择","link":"#_2-3-2-特征选择","children":[]},{"level":3,"title":"2.3.3-特征提取","slug":"_2-3-3-特征提取","link":"#_2-3-3-特征提取","children":[]},{"level":3,"title":"类别可分离性判据","slug":"类别可分离性判据","link":"#类别可分离性判据","children":[]}]}],"git":{"createdTime":1667841430000,"updatedTime":1769407912000,"contributors":[{"name":"233Mac","username":"233Mac","email":"ayusummer233@vip.qq.com","commits":4,"url":"https://github.com/233Mac"},{"name":"咸鱼型233","username":"咸鱼型233","email":"ayusummer233@qq.com","commits":1,"url":"https://github.com/咸鱼型233"}]},"readingTime":{"minutes":0.94,"words":281},"filePathRelative":"AI/概念与基础/机器学习/MachineLearning.md","localizedDate":"2022年11月7日","excerpt":"","autoDesc":true}');export{m as comp,p as data};
