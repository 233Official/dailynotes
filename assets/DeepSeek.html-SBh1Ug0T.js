import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as l,a,o}from"./app-DX9gUdjC.js";const r={};function n(p,e){return o(),l("div",null,e[0]||(e[0]=[a('<h1 id="deepseek" tabindex="-1"><a class="header-anchor" href="#deepseek"><span>DeepSeek</span></a></h1><blockquote><p><a href="https://chat.deepseek.com/" target="_blank" rel="noopener noreferrer">DeepSeek</a></p></blockquote><hr><h2 id="使用" tabindex="-1"><a class="header-anchor" href="#使用"><span>使用</span></a></h2><ul><li><p>网页使用</p><ul><li><a href="https://chat.deepseek.com/" target="_blank" rel="noopener noreferrer">DeepSeek</a></li><li><a href="https://cloud.siliconflow.cn/models" target="_blank" rel="noopener noreferrer">硅基流动</a></li></ul></li><li><p>API 使用</p><ul><li>NextChat + Deepseek: [<a href="https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/issues/4643" target="_blank" rel="noopener noreferrer">Feature Request]: 请教怎样在NextChat中使用Deepseek V2的API · Issue #4643 · ChatGPTNextWeb/ChatGPT-Next-Web · GitHub</a></li></ul></li><li><p>实用集成</p><p><a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/README_cn.md" target="_blank" rel="noopener noreferrer">awesome-deepseek-integration/README_cn.md at main · deepseek-ai/awesome-deepseek-integration · GitHub</a></p><ul><li>结合 VSCode 使用: <a href="https://github.com/deepseek-ai/awesome-deepseek-integration/blob/main/docs/continue/README_cn.md" target="_blank" rel="noopener noreferrer">awesome-deepseek-integration/docs/continue/README_cn.md at main · deepseek-ai/awesome-deepseek-integration · GitHub</a></li><li>本地 Ollama + DeepSeek: <a href="https://www.zzfly.net/ollama-deepseek-copilot/" target="_blank" rel="noopener noreferrer">使用Ollama搭建本地的 AI Copilot 编程助手 - By烟花易冷 (zzfly.net)</a></li></ul></li></ul><hr><p><img src="http://cdn.ayusummer233.top/DailyNotes/202503041039330.png" alt="image-20250304103952175"></p><ul><li><p><code>System Prompt</code>: 对话开始时给模型的“初始指令”，用于定义模型的角色、回答风格或任务目标</p><p>例如：“你是一个专业翻译，请将中文翻译成英文”或“用简单易懂的语言解释概念”。</p><p>需要避免开放式指令，否则模型可能偏离预期。</p></li><li><p><code>Max Tokens（最大生成长度）</code>: 限制模型单次生成的最大文本长度（1 Token ≈ 1个英文单词或3-4个中文字）。</p><p>不同模型有总Token上限（输入+输出），例如：</p><ul><li>GPT-3.5：通常为 <code>4096 Tokens</code></li><li>GPT-4：可达 <code>8192 Tokens</code> 或更高</li></ul><p>设置建议</p><ul><li>短回答（快速回复）：<code>100-300</code> tokens</li><li>长文本（文章/故事）：<code>500-1000+</code> tokens</li><li>注意：过长可能导致回答冗余，过短可能截断内容。</li></ul></li><li><p><code>Temperature（温度）</code>: 控制输出的随机性。值越高，回答越多样/有创意；值越低，回答越保守/稳定。</p><p>设置建议</p><ul><li>严谨场景（如代码、翻译）：<code>0.2-0.5</code></li><li>创意场景（如写诗、故事）：<code>0.7-1.0</code></li><li>极端随机：<code>&gt;1.0</code>（可能导致语句不通）</li></ul></li><li><p><code>Top-P（核采样）</code>: 从累积概率超过阈值P的词中选择候选词（动态调整候选词数量）</p><p>设置建议</p><ul><li>平衡多样性和质量：<code>0.7-0.9</code></li><li>高确定性回答：<code>0.3-0.5</code></li><li>通常与Temperature配合使用。</li></ul></li><li><p><code>Top-K（前K个候选）</code>: 仅从概率最高的前K个词中选择，限制候选词数量</p><p>设置建议</p><ul><li>限制输出范围：<code>50-100</code>（常用）</li><li>严格筛选：<code>10-20</code>（可能过于机械）</li><li>与Top-P二选一，通常优先用Top-P。</li></ul></li><li><p><code>Frequency Penalty（频率惩罚）</code>: 控制模型生成重复词汇的惩罚力度。值越高，模型越倾向于避免重复使用已生成的词汇；值越低，允许的重复度越高</p><p>通过降低已出现词汇的概率权重，减少重复（例如避免“很好很好很好”这类冗余表达）。</p><p><strong>取值范围</strong>通常为 <code>0.0~2.0</code>（不同平台可能略有差异），默认值为 <code>0</code>（无惩罚）。</p></li></ul><p><strong>参数搭配建议</strong>：</p><table><thead><tr><th style="text-align:center;"><strong>场景</strong></th><th style="text-align:center;">Temperature</th><th style="text-align:center;">Top-P</th><th style="text-align:center;">Max Tokens</th></tr></thead><tbody><tr><td style="text-align:center;">技术文档生成</td><td style="text-align:center;">0.3</td><td style="text-align:center;">0.5</td><td style="text-align:center;">500</td></tr><tr><td style="text-align:center;">创意故事写作</td><td style="text-align:center;">0.8</td><td style="text-align:center;">0.9</td><td style="text-align:center;">800</td></tr><tr><td style="text-align:center;">客服问答</td><td style="text-align:center;">0.2</td><td style="text-align:center;">0.3</td><td style="text-align:center;">200</td></tr><tr><td style="text-align:center;">开放域闲聊</td><td style="text-align:center;">0.7</td><td style="text-align:center;">0.8</td><td style="text-align:center;">300</td></tr></tbody></table><hr><h2 id="本地部署-deepseek" tabindex="-1"><a class="header-anchor" href="#本地部署-deepseek"><span>本地部署 DeepSeek</span></a></h2><blockquote><p><a href="https://blog.lovefc.cn/archives/start.html" target="_blank" rel="noopener noreferrer">本地部署deepseek模型保姆级教程</a></p></blockquote><blockquote><p>TODO: 本文详细介绍了如何在本地部署DeepSeek模型，特别是针对Windows系统的用户。文章首先强调了显卡的重要性，并提供了推荐的硬件配置。接着，文章逐步指导用户如何下载并运行Ollama，设置环境变量以避免C盘空间不足的问题。随后，文章介绍了如何通过Ollama下载和运行DeepSeek模型，并提供了不同模型对显存的需求和推荐显卡。最后，文章推荐了几种可视化AI工具，如Chatbox和Next.js Ollama LLM UI，以便用户更方便地使用本地模型。整个教程旨在帮助用户快速、简单地在本地搭建和使用DeepSeek模型。</p><p>目前直接使用联网API或者Web页面聊天的体验都不错，自己本地部署也不是满血，也没有业务需求，暂时没有部署的理由，暂且搁置，后续有需求了再看看</p></blockquote><blockquote><p><a href="https://www.zzfly.net/ollama-deepseek-copilot/" target="_blank" rel="noopener noreferrer">使用Ollama搭建本地的 AI Copilot 编程助手 - By烟花易冷 (zzfly.net)</a></p><p><a href="https://www.cnblogs.com/NBeveryday/p/18742347/ollama" target="_blank" rel="noopener noreferrer">Mac本地部署DeepSeek(简洁版)</a></p></blockquote><hr>',16)]))}const s=t(r,[["render",n],["__file","DeepSeek.html.vue"]]),d=JSON.parse('{"path":"/AI/%E5%B7%A5%E5%85%B7%E4%B8%8E%E5%AE%9E%E8%B7%B5/%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%B9%B3%E5%8F%B0/DeepSeek.html","title":"DeepSeek","lang":"zh-CN","frontmatter":{"category":["AI","大模型"],"tags":["DeepSeek"],"description":"DeepSeek DeepSeek 使用 网页使用 DeepSeek 硅基流动 API 使用 NextChat + Deepseek: [Feature Request]: 请教怎样在NextChat中使用Deepseek V2的API · Issue #4643 · ChatGPTNextWeb/ChatGPT-Next-Web · GitHub 实...","head":[["meta",{"property":"og:url","content":"https://233official.github.io/dailynotes/AI/%E5%B7%A5%E5%85%B7%E4%B8%8E%E5%AE%9E%E8%B7%B5/%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%B9%B3%E5%8F%B0/DeepSeek.html"}],["meta",{"property":"og:site_name","content":"DailyNotes"}],["meta",{"property":"og:title","content":"DeepSeek"}],["meta",{"property":"og:description","content":"DeepSeek DeepSeek 使用 网页使用 DeepSeek 硅基流动 API 使用 NextChat + Deepseek: [Feature Request]: 请教怎样在NextChat中使用Deepseek V2的API · Issue #4643 · ChatGPTNextWeb/ChatGPT-Next-Web · GitHub 实..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"http://cdn.ayusummer233.top/DailyNotes/202503041039330.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2026-01-26T06:11:52.000Z"}],["meta",{"property":"article:tag","content":"DeepSeek"}],["meta",{"property":"article:modified_time","content":"2026-01-26T06:11:52.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"DeepSeek\\",\\"image\\":[\\"http://cdn.ayusummer233.top/DailyNotes/202503041039330.png\\"],\\"dateModified\\":\\"2026-01-26T06:11:52.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"咸鱼型233\\",\\"url\\":\\"https://233official.github.io/dailynotes/\\"}]}"]],"date":"2025-03-04T06:48:51.000Z"},"headers":[{"level":2,"title":"使用","slug":"使用","link":"#使用","children":[]},{"level":2,"title":"本地部署 DeepSeek","slug":"本地部署-deepseek","link":"#本地部署-deepseek","children":[]}],"git":{"createdTime":1741070931000,"updatedTime":1769407912000,"contributors":[{"name":"233Mac","username":"233Mac","email":"ayusummer233@vip.qq.com","commits":2,"url":"https://github.com/233Mac"}]},"readingTime":{"minutes":3.31,"words":993},"filePathRelative":"AI/工具与实践/模型与平台/DeepSeek.md","localizedDate":"2025年3月4日","excerpt":"","autoDesc":true}');export{s as comp,d as data};
